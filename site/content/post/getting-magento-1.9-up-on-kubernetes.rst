---
title: "Getting Magento 1.9x up on Kubernetes (1.3)"
description: "An in depth guide on Kubernetes for Magento developers"
date: "2016-07-22"
---

Getting Magento 1.9x up on Kubernetes
=====================================

I have been playing quite a bit recently on Kubernetes, and gotten various
applications such as Sensu, NGINX, Hugo, Prometheus and a few others up and
going on it. This is what amounts to one way to get Magento up on Kubernetes,
on GKE. This guide is written for Magento developers looking to gain familiarity
with Kubernetes, and perhaps Docker. If there are terms that you are unfamiliar
with, or perhaps something I have worded badly, please let me know and I'll
fix it up.

Presumptions
------------

- I've previously set up the cluster, and the kubectl cli tool.
- I've configured the kubectl tool to use the appropriate namespace.
- Your Kubernetes cluster already has the DNS server set up
- Your Kubernetes cluster already has Prometheus set up (a blog post for another
 day!)

The problem (or why should I care)
----------------------------------

Magento (the way I like to run it) consists of a few moving parts:

#. PHP
#. NGINX
#. Redis
#. MySQL

Each of those parts have to run somewhere *reliably* in a known location. There
might be several copies of these parts (most commonly the PHP/NGINX components)
for redundancy, in case requests fail or get blocked. Further, in order to make
any ongoing changes to the stack, we need to be able to quickly reason about
exactly what's running, including versions, where the processes are running,
how much resource they're using and when they were last modified.

A combination of Kubernetes and Docker serve to solve a lot of those problems
elegantly. Docker images (run by the Docker daemon) are encapsulated,
distributable environments to run a process in. They roughly consist of the
following:

- A separate spot in the kernel to execute the process in.
- A root file system (including the application) to run things in.
- Reasonable isolation from other processes running on the same kernel.

Because they're encapsulated, the runtime environment is largely the same from
development to staging to production, and beyond. This encapsulation solves one
of the hardest problems: knowing exactly what's running.

Kubernetes provides a system to manage and distribute these Docker images among
a series of machines, without caring about exactly where the image is running or
setting up networking to that image. It manages the entire cluster, and is
responsible for:

- Managing how much available compute resources there are in the entire cluster
- Deciding which machine a process should run on
- Starting and monitoring that process on the machine
- Creating a means to route to that process
- Provisioning any required cloud resources needed by that process instance
- Handling the failure of a machine

Kubernetes is entirely declarative, and is responsible for enforcing the state
of your environment. It provides a superb abstraction between "the things the
developer should know about" and "the gory detail the sysadmin needs to know
about". We're going to use it because it dramatically reduces the impact of
failure for any one component. If a process dies, it's restarted. If a machine
dies, work is scheduled elsewhere. If Kubernetes dies, you're screwed -- but
at that point, you were screwed anyway.

Getting Started
---------------

Our own, tidy workshop
""""""""""""""""""""""

Most of the resources in Kubernetes operate in the context of a namespace.
A namespace prevents collisions between applications that need to be discovered,
lets us sets some resource limits and (coming soon) network policy. To provision
anything, we have to have a namespace to put it.
`Check out the namespace docs for more information.`_

Kubernetes creates resources based on text configuration, in either JSON or
Yaml. I like Yaml, so we're going to use that. So, let's get started, and
create a namespace resource. Create a file called `20-m1onk8s-littleman-co.ns.yml`
with the following content:

.. Code:: yaml

  ---
  # Generated by Boilr at Wed, 20 Jul 2016 20:50:37 AEST
  apiVersion: "v1"
  kind: "Namespace"
  metadata:
    name: "m1onk8s-littleman-co"
    # See http://blog.kubernetes.io/2016/04/Kubernetes-Network-Policy-APIs.html
    # net.alpha.kubernetes.io/network-isolation: "off"

You'll notice a few things about this file:

#. It's got a comment that indicates it's generated. I'm too lazy to generate
   them myself, so I use a templating tool called `boilr`_. If you like, the
   templates are available on `the littleman.co GitHub`_.
#. "`net.alpha.kubernetes.io/network-isolation`_: "off"" is commented out. Alpha
   resources are not available on GKE; when this feature is beta, I'll try and
   remember to update this.
#. The file is prefixed with the number 20. We're applying lots of configuration
   at once, and this number determines what order to apply the configuration in.

Simple Applications
"""""""""""""""""""

There are pre-build images for MySQL and Redis that can be deployed as is, and
require very little effort on the part of the developer. We're going to start
ith those, as Kubernetes has quite the learning curve, and it's nice to start
slow.

The way I like to get applications running on Kubernetes is to have:

- A `deployment`_ artifact: Something to indicate what to run on Kubernetes,
  and how many copies.
- A `service`_ artifact: Something to indicate how to route things on the
  network, and to where.

We'll start with the deployment. The deployment I'm using is below. I've heavily
commented it, to explain what each constituent part is for. Create a file
called `50-cache.dep.yaml`, and paste in the below.

# Todo: Make a note about what a pod is, and that the deployment is creating
# and managing some.

.. Code:: yaml

  ---
  # Generated by boilr at Wed, 20 Jul 2016 20:55:37 AEST
  # Kubernetes separates its artifacts into revisions, with Alpha, Beta, and
  # standard. The apiVersion tag specifies where Kubernetes should look for this
  # object definition.
  apiVersion: "extensions/v1beta1"
  # Well, it's a deployment, as mentioned.
  kind: "Deployment"
  metadata:
    # The labels are used for grouping tasks of resource, such as for service
    # discovery later.
    labels:
      application: "redis"
      role: "cache"
    # How to reference this resource going forward
    name: "cache"
    # Where to put this resource
    namespace: "m1onk8s-littleman-co"
  spec:
    # How many instances of the application we want to run on the cluster. All
    # applications can be horizontally scaled, however, in this case we're
    # running a stateful Redis instance, and it's not so easy to scale. We'll
    # stick to one.
    replicas: 1
    selector:
      # The deployment artifact will create a "replica set", which manages how
      # many pods are running at any given time. Kubernetes matches the labels
      # of the pods, defined later, with these pods to reconcile the pods the
      # replica set is looking for with the pods in the cluster.
      matchLabels:
        application: "redis"
        role: "cache"
    strategy:
      # How to push new versions of the application. In this case, we're
      # allowing {n}+- 1 container, where n = 1 (defined earlier in replicas).
      # Rolling Update is the only supported deployment mechanism at the moment.
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: "RollingUpdate"
    template:
      metadata:
        labels:
          application: "redis"
          role: "cache"
      # Kubernetes deploys {n} containers together, who all share an IP address.
      #  This allows us to do things like attach monitoring processes to our
      # application processes, or attach PHP to NGINX.
      # In this case, we're going to have the root process (redis) and a redis
      # metric exporter for Prometheus.
      # Todo: Add the redis sidecar exporter: pull 21zoo/redis_exporter
      spec:
        # Todo: Actually define a PD to keep state
        # Here's where we declare the type of storage resources that our pod
        # will need. Kubernetes allows us to use a variety of storage
        # abstractions as volumes in our container, including configuration,
        # gluster, GCE Persistent Disks and more.
        # volumes:
        # - name: "cache-etc-conf-d"
        # configMap:
        #   name: "cache-etc-conf-d"
        # - name: "cache-data"
        #   hostPath:
        #     path: /data/cache/
        containers:
          # Our application! Here, we're running the official redis:3.2.1-alpine
          # container. There's not much to it, except to say that it's a redis
          # instance running on the Apline Linux root filesystem.
        - name: "redis"
          # The docker image to use
          image: "redis:3.2.1-alpine"
          # Kubernetes will automatically pull the image onto the node that
          # needs to run it. However, if you use the same docker image tag
          # (for example, 'latest') and update the image, Kubernetes won't
          # check back upstream unless you tell it with "imagePullPolicy:
          # Always". Note: I think this is a tremendously bad idea, as
          # different images will be updated at different times.
          imagePullPolicy: "IfNotPresent"
          # Each node has a finite amount of resource, and each application
          # uses an amount of resource. We should (in theory) have a good idea
          # how much resource each instance of our application will require.
          # The below configuration allows us to "reserve" the resources
          # required - In this case, 100m (.1) of a CPU, and 64mb of ram. I'm
          # not too sure what the difference is yet - We're learning about
          # this together.
          resources:
            limits:
              cpu: "100m"
              memory: "64Mi"
            requests:
              cpu: "100m"
              memory: "64Mi"
          # These are the ports to make available on the container. When we
          # create a service, we'll be directing traffic to these ports.
          ports:
          - containerPort: 6379
            protocol: "TCP"
            name: "redis"
          # The below configuration tells Kubernetes to attach the persistent
          # storage we requested earlier to this container.
          # Todo: Attach the PD.
          # volumeMounts:
          # - name: "cache-etc-conf-d"
          #   readOnly: true
          #   mountPath: "/etc/cache/conf.d"
          # - name: "cache-data
          #   readOnly: false
          #   mountPath: "/data"
          # Kubernetes provisions a container, but there's a period between
          # "process has been started" and "application is ready". We dont want
          # to send traffic to this application instance before its ready, so
          # we periodically check its readiness by testing if port 6379 is open
          readinessProbe:
            tcpSocket:
              port: 6379
            initialDelaySeconds: 1
            timeoutSeconds: 5
          # During the lifecycle of the application, something might go wrong.
          # Redis, for example, could become blocked and refuse to serve any
          # more traffic. We don't want traffic being sent to an unhealthy
          # application instance! To avoid this, we check if the application
          # is healthy every so often, by testing if port 6379 is open.
          livenessProbe:
            tcpSocket:
              port: 6379
            initialDelaySeconds: 1
            timeoutSeconds: 5
        # Kubernetes will automatically restart containers when it detects they
        # are unhealthy, either by failling the liveness probe or the process
        # exiting. We usually went the application restarted, so we indicate
        # this to Kubernetes with a `restartPolicy`
        restartPolicy: "Always"
        # I have no idea what this does. When I do, I'll update these notes!
        securityContext: {}

Whoa. That was a tonne of information! Luckily, I reckon that's the most
complicated artifact that we're going to deal with for a very long time. Further,
there's a bunch of reoccurring themes that make Kubernetes easiest to digest
over time. Kind of like Magento!

So, we have a Redis instance running. We can check this by querying the
Kubernetes API for the status of that pod

.. Code:: bash

  $ kubectl get pods
  NAME                     READY     STATUS    RESTARTS   AGE
  cache-4036923991-vwy3z   1/1       Running   0          22h

There it is! Let's take a closer look:

.. Code:: bash

  $ kubectl describe pod cache-4036923991-vwy3z
  Name:		cache-4036923991-vwy3z
  Namespace:	m1onk8s-littleman-co
  Node:		{node-name}/10.240.0.2
  ...

It'll show you a bunch more information. But, it doesn't show us how how to find
our application in the network!

.. container:: tip warning

  It does show an IP. Don't use it - it's tied to the application instance, and
  not permanent.

Kubernetes provides a means to handle the discovery and routing of applications
for us, called "services". Services are a pointer to a set of applications -
they provide a fixed address at which you can query an instance of an
application.

To create a service we need a service declaration file. Create a file called
`50-cache.svc.yml`, and paste in the content below:

.. Code:: yaml

  ---
  # Generated by boilr at Thu, 21 Jul 2016 20:00:17 AEST
  kind: "Service"
  apiVersion: "v1"
  metadata:
    # The name will form the first part of the URL that we can find our service
    # at.
    name: "cache"
    # The namespace is the same namespace we specified earlier, and will form
    # the next part of the URL we will query
    namespace: "m1onk8s-littleman-co"
    annotations:
      # I like monitoring services with Prometheus. This means "Find and scrape"
      # this endpoint for metrics
      prometheus.io/scrape: "true"
    labels:
      # These labels are how this service decides what to route traffic to. They
      # should be a matching set as the ones defined in the deployment earlier.
      # Note: These labels work on an "everything that matches" basis. If you
      # have another service that routes to "applicaton: redis", it will Also
      # match the same pods as this service.
      application: "redis"
      role: "cache"
  spec:
    selector:
      # See above.
      application: "redis"
      role: "cache"
    ports:
      # Which ports to route traffic for. These should be the same as the sum
      # of all ports opened by all containers in the port.
      - protocol: "TCP"
        name: "redis"
        port: 6379
      # Todo: Put promethus here.
      # - protocol: "TCP"
      #   name: "another-redis"
      #   port: 30redis
    type: "ClusterIP"

.. container:: tip info

  What if you have more then one instance of an application (replica)?
  Kubernetes will route all of them, load balancing between them in a round
  robin.

Now we have the two Kubernetes definitions:

- `20-m1onk8s-littleman-co.ns.yml`
- `50-cache.dep.yml`
- `50-cache.svc.yml`

Making changes in each one and then applying them can get tiresome. Luckily,
we don't have to do that! Kubernetes will simpily patch the resources that are
there if you ask it to, updating them as required. We can even patch the entire
set of resources at once! This is super nice if you're working with lots of
files, as we will be later.

.. Code:: bash

  # Note: The definition files must be the only thing in the directory for this
  # to work
  $ cd {directory you created the files in}
  $ kubectl apply -f .

  namespace "m1onk8s-littleman-co" configured
  deployment "cache" configured
  service "cache" created

Whoo! Looks like everything worked OK. However, how do we know our service is
working? Let's take a look:

.. Code:: bash

  $ kubectl get svc
  NAME      CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
  cache     10.59.254.85   <none>        6379/TCP   40s

Yup. Kubenretes has found it. It's not an externally facing service, so that
`<none>` is fine. However, is it working? Let's check:

.. Code:: bash

  $ kubectl describe svc cache

  Name:			cache
  Namespace:		m1onk8s-littleman-co
  Labels:			application=redis,role=cache
  Selector:		application=redis,role=cache
  Type:			ClusterIP
  IP:			10.59.254.85
  Port:			redis	6379/TCP
  Endpoints:		10.56.0.7:6379 # <-- The pod
  Session Affinity:	None
  No events.

See the bit there called `Endpoints` and the IP next to it? That's the pod we
started earlier! Looks like everything is working. However, that's not a good
text - We know it's found the pod, and we know that the pod has port 6379 open
(thanks to the earlier liveness checks). However, is Redis actually working?

Well, we could query it with the Redis-cli tool. But wait - What do we query?
There is two things:

- The service IP
- The domain name

We're going to do the latter, as it's simpler, and reliable across clusters
and service creation. Kubernetes can run an additional DNS service - most
clusters have this enabled by default. The DNS service some information about the
service, and turns it into a domain name. The domain names are constructed as
follows:

.. Code::

  {pod-name}.{namespace}.svc.{cluster-domain}}

The domain suffix is configured when the cluster is created. On GKE, mine was
`cluster.local` - To find yours, take a look at the options the kubelet was
started with, or consult the cluster manual.

In our case, this means our DNS entry will be

.. Code::

  cache.m1onk8s-littleman-co.svc.cluster.local

However, we don't need to enter all that. Kubernetes modifies the nameserver
resolution behaviour such that, within this namespace, any of the following
values are acceptable:

- `cache`
- `cache.m1onk8s-littleman-co`
- `cache.m1onk8s-littleman-co.svc`
- `cache.m1onk8s-littleman-co.svc.cluster.local`

Unfortunately, there's no way to connect directly to the service from inside
the cluster. However, we can create a short lived pod just to test the
connection[1]_. We're going to use the same redis image as we're running
the server on, as it has the `redis-cli` tool, and is already on at least
one node.

.. Code:: bash

  $ kubectl run -i --tty redis --image=redis:3.2.1-alpine --restart=Never sh

  Waiting for pod m1onk8s-littleman-co/redis-2j2vx to be running, status is Pending, pod ready: false

  Hit enter for command prompt
  # Hit enter

  # The prompt looks like '{dir} #'
  /data # redis-cli -h cache

  cache:6379>

Yeah! Looks like we're connected. Redis is up and running! You can just exit
that pod, and it'll be disposed of.

I'm going to leave it here for right now.

.. _boilr: https://github.com/boilr
.. _Check out the namespace docs for more information.: http://kubernetes.io/docs/user-guide/namespaces/
.. _deployment: http://kubernetes.io/docs/user-guide/deployments/
.. _service: http://kubernetes.io/docs/user-guide/services/
.. _net.alpha.kubernetes.io/network-isolation: http://blog.kubernetes.io/2016/04/Kubernetes-Network-Policy-APIs.html
.. _the littleman.co GitHub: https://github.com/littlemanco/

Referenecs
----------

I learned things during this too! I had previously never applied resource limits
for example.

- http://kubernetes.io/docs/admin/resourcequota/walkthrough/
- http://kubernetes.io/docs/user-guide/managing-deployments/
.. [1] http://kubernetes.io/docs/user-guide/kubectl/kubectl_run/

Things I intend to cover (or, todo)
-----------------------------------

- Metrics
- Logs
- Resource Allocations
