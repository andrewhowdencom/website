=====================================
Getting Magento 1.9x up on Kubernetes
=====================================

I have been playing quite a bit recently on Kubernetes, and gotten various applications such as Sensu, Nginx, Hugo, Prometheus and a few others up and going on it. This is what amounts to one way to get Magento up on Kubernetes, on GKE. This guide is written for Magento developers looking to gain familiarity with Kubernetes, and perhaps Docker. If there are terms that you are unfamiliar with, or perhaps something I have worded badly, please let me know and I'll fix it up.

Things I intend to cover (or, todo)
-----------------------------------

- Metrics
- Logs
- Resource Allocations

Presumptions
------------

- I've previously set up the cluster, and the kubectl cli tool.
- I've configured the kubectl tool to use the appropriate namespace.

The problem (or why should I care)
----------------------------------

Magento (the way I liketo run it) consists of a few moving parts:

#. PHP
#. NGINX
#. Redis
#. MySQL

Each of those parts have to run somewhere *reliably* in a known location. There might be several copies of these parts (most commonly the PHP/NGINX components) for redundancy, in case requests fail or get blocked. Further, in order to make any ongoing changes to the stack, we need to be able to quickly reason about exactly what's running, including versions, where the processes are running, how much resource they're using and when they were last modified.

A combination of Kubernetes and Docker serve to solve a lot of those problems elegantly. Docker images (run by the Docker daemon) are encapsulated, distributable environments to run a process in. They roughly consist of the following:

- A separate spot in the kernel to execute the process in.
- A root file system (including the application) to run things in.
- Reasonable isolation from other processes running on the same kernel.

Because they're encapsulated, the runtime environment is largely the same from development to staging to production, and beyond. This encapsulation solves one of the hardest problems: knowing exactly what's running.

Kubernetes provides a system to manage and distribute these Docker images among a series of machines, without caring about exactly where the image is running or setting up networking to that image. It manages the entire cluster, and is responsible for:

- Managing how much available compute resources there are in the entire cluster
- Deciding which machine a process should run on
- Starting and monitoring that process on the machine
- Creating a means to route to that process
- Provisioning any required cloud resources needed by that process instance
- Handling the failure of a machine

Kubernetes is entirely declarative, and is responsible for enforcing the state of your environment. It provides a superb abstraction between "the things the developer should know about" and "the gory detail the sysadmin needs to know about". We're going to use it because it dramatically reduces the impact of failure for any one component. If a process dies, it's restarted. If a machine dies, work is scheduled elsewhere. If Kubernetes dies, you're screwed -- but at that point, you were screwed anyway.

Getting Started
---------------

Our own, tidy workshop
""""""""""""""""""""""

Most of the resources in Kubernetes operate in the context of a namespace. A namespace prevents collisions between applications that need to be discovered, lets us sets some resource limits and (coming soon) network policy. To provision anything, we have to have a namespace to put it. `Check out the namespace docs for more information.`_

Kubernetes creates resources based on text configuration, in either JSON or Yaml. I like Yaml, so we're going to use that. So, let's get started, and create a namespace resource. Create a file called `20-m1onk8s-littleman-co.ns.yml` with the following content:

.. Code:: yaml

  ---
  # Generated by Boilr at Wed, 20 Jul 2016 20:50:37 AEST
  apiVersion: "v1"
  kind: "Namespace"
  metadata:
    name: "m1onk8s-littleman-co"
    # See http://blog.kubernetes.io/2016/04/Kubernetes-Network-Policy-APIs.html
    # net.alpha.kubernetes.io/network-isolation: "off"

You'll notice a few things about this file:

#. It's got a comment that indicates it's generated. I'm too lazy to generate them myself, so I use a templating tool called `boilr`_. If you like, the templates are available on `the littleman.co GitHub`_.
#. "`net.alpha.kubernetes.io/network-isolation`_: "off"" is commented out. Alpha resources are not available on GKE; when this feature is beta, I'll try and remember to update this.
#. The file is prefixed with the number 20. We're applying lots of configuration at once, and this number determines what order to apply the configuration in.

Simple Applications
"""""""""""""""""""

There are pre-build images for MySQL and Redis that can be deployed as is, and require very little effort on the part of the developer. We're going to start with those, as Kubernetes has quite the learning curve, and it's nice to start slow.

The way I like to get applications running on Kubernetes is to have:

- A `deployment`_ artifact: Something to indicate what to run on Kubernetes, and how many copies.
- A `service`_ artifact: Something to indicate how to route things on the network, and to where.

We'll start with the deployment. The deployment I'm using is below. I've heavily commented it, to explain what each constituent part is for.

.. Code:: yaml

  ---
  # Generated by boilr at Wed, 20 Jul 2016 20:55:37 AEST
  # Kubernetes separates its artifacts into revisions, with Alpha, Beta, and
  # standard. The apiVersion tag specifies where Kubernetes should look for this
  # object definition.
  apiVersion: "extensions/v1beta1"
  # Well, it's a deployment, as mentioned.
  kind: "Deployment"
  metadata:
    # The labels are used for grouping tasks of resource, such as for service
    # discovery later.
    labels:
      application: "redis"
      role: "cache"
    # How to reference this resource going forward
    name: "cache"
    # Where to put this resource
    namespace: "m1onk8s-littleman-co"
  spec:
    # How many instances of the application we want to run on the cluster. All
    # applications can be horizontally scaled, however, in this case we're
    # running a stateful Redis instance, and it's not so easy to scale. We'll
    # stick to one.
    replicas: 1
    selector:
      # The deployment artifact will create a "replica set", which manages how
      # many pods are running at any given time. Kubernetes matches the labels
      # of the pods, defined later, with these pods to reconcile the pods the
      # replica set is looking for with the pods in the cluster.
      matchLabels:
        application: "redis"
        role: "cache"
    strategy:
      # How to push new versions of the application. In this case, we're 
      # allowing {n}+- 1 container, where n = 1 (defined earlier in replicas).
      # Rolling Update is the only supported deployment mechanism at the moment.
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: "RollingUpdate"
    template:
      metadata:
        labels:
          application: "redis"
          role: "cache"
      # Kubernetes deploys {n} containers together, who all share an IP address.
      #  This allows us to do things like attach monitoring processes to our
      # application processes, or attach PHP to NGINX.
      # In this case, we're going to have the root process (redis) and a redis
      # metric exporter for Prometheus.
      # Todo: Add the redis sidecar exporter: pull 21zoo/redis_exporter
      spec:
        # Todo: Actually define a PD to keep state
        # Here's where we declare the type of storage resources that our pod
        # will need. Kubernetes allows us to use a variety of storage
        # abstractions as volumes in our container, including configuration,
        # gluster, GCE Persistent Disks and more.
        # volumes:
        # - name: "cache-etc-conf-d"
        # configMap:
        #   name: "cache-etc-conf-d"
        # - name: "cache-data"
        #   hostPath:
        #     path: /data/cache/
        containers:
          # Our application! Here, we're running the official redis:3.2.1-alpine
          # container. There's not much to it, except to say that it's a redis
          # instance running on the Apline Linux root filesystem.
        - name: "redis"
          # The docker image to use
          image: "redis:3.2.1-alpine"
          # Kubernetes will automatically pull the image onto the node that
          # needs to run it. However, if you use the same docker image tag
          # (for example, 'latest') and update the image, Kubernetes won't
          # check back upstream unless you tell it with "imagePullPolicy:
          # Always". Note: I think this is a tremendously bad idea, as
          # different images will be updated at different times.
          imagePullPolicy: "IfNotPresent"
          # Each node has a finite amount of resource, and each application
          # uses an amount of resource. We should (in theory) have a good idea
          # how much resource each instance of our application will require.
          # The below configuration allows us to "reserve" the resources
          # required - In this case, 100m (.1) of a CPU, and 64mb of ram. I'm
          # not too sure what the difference is yet - We're learning about
          # this together.
          resources:
            limits:
              cpu: "100m"
              memory: "64Mi"
            requests:
              cpu: "100m"
              memory: "64Mi"
          # These are the ports to make available on the container. When we
          # create a service, we'll be directing traffic to these ports.
          ports:
          - containerPort: 6379
            protocol: "TCP"
            name: "redis"
          # The below configuration tells Kubernetes to attach the persistent
          # storage we requested earlier to this container.
          # Todo: Attach the PD.
          # volumeMounts:
          # - name: "cache-etc-conf-d"
          #   readOnly: true
          #   mountPath: "/etc/cache/conf.d"
          # - name: "cache-data
          #   readOnly: false
          #   mountPath: "/data"
          # Kubernetes provisions a container, but there's a period between
          # "process has been started" and "application is ready". We dont want
          # to send traffic to this application instance before its ready, so
          # we periodically check its readiness by testing if port 6379 is open
          readinessProbe:
            tcpSocket:
              port: 6379
            initialDelaySeconds: 1
            timeoutSeconds: 5
          # During the lifecycle of the application, something might go wrong.
          # Redis, for example, could become blocked and refuse to serve any
          # more traffic. We don't want traffic being sent to an unhealthy
          # application instance! To avoid this, we check if the application
          # is healthy every so often, by testing if port 6379 is open.
          livenessProbe:
            tcpSocket:
              port: 6379
            initialDelaySeconds: 1
            timeoutSeconds: 5
        # Kubernetes will automatically restart containers when it detects they
        # are unhealthy, either by failling the liveness probe or the process
        # exiting. We usually went the application restarted, so we indicate
        # this to Kubernetes with a `restartPolicy`
        restartPolicy: "Always"
        # I have no idea what this does. When I do, I'll update these notes!
        securityContext: {}


.. _boilr: https://github.com/boilr
.. _Check out the namespace docs for more information.: http://kubernetes.io/docs/user-guide/namespaces/
.. _deployment: http://kubernetes.io/docs/user-guide/deployments/
.. _service: http://kubernetes.io/docs/user-guide/services/
.. _net.alpha.kubernetes.io/network-isolation: http://blog.kubernetes.io/2016/04/Kubernetes-Network-Policy-APIs.html
.. _the littleman.co GitHub: https://github.com/littlemanco/

Referenecs
----------

I learned things during this too! I had previously never applied resource limits for example.

- http://kubernetes.io/docs/admin/resourcequota/walkthrough/
- http://kubernetes.io/docs/user-guide/managing-deployments/
