=====================================
Getting Magento 1.9x up on Kubernetes
=====================================

I have been playing quite a bit recently on Kubernetes, and gotten various applications such as Sensu, Nginx, Hugo, Prometheus and a few others up and going on it. This is what amounts to one way to get Magento up on Kubernetes, on GKE. This guide is written for Magento developers looking to gain familiarity with Kubernetes, and perhaps Docker. If there are terms that you are unfamiliar with, or perhaps something I have worded badly, please let me know and I'll fix it up.

Things I intend to cover (or, todo)
-----------------------------------

- Metrics
- Logs
- Resource Allocations

Presumptions
------------

- I've previously set up the cluster, and the kubectl cli tool.
- I've configured the kubectl tool to use the appropriate namespace.

The problem (or why should I care)
----------------------------------

Magento (the way I liketo run it) consists of a few moving parts:

#. PHP
#. NGINX
#. Redis
#. MySQL

Each of those parts have to run somewhere *reliably* in a known location. There might be several copies of these parts (most commonly the PHP/NGINX components) for redundancy, in case requests fail or get blocked. Further, in order to make any ongoing changes to the stack, we need to be able to quickly reason about exactly what's running, including versions, where the processes are running, how much resource they're using and when they were last modified.

A combination of Kubernetes and Docker serve to solve a lot of those problems elegantly. Docker images (run by the Docker daemon) are encapsulated, distributable environments to run a process in. They roughly consist of the following:

- A separate spot in the kernel to execute the process in.
- A root file system (including the application) to run things in.
- Reasonable isolation from other processes running on the same kernel.

Because they're encapsulated, the runtime environment is largely the same from development to staging to production, and beyond. This encapsulation solves one of the hardest problems: knowing exactly what's running.

Kubernetes provides a system to manage and distribute these Docker images among a series of machines, without caring about exactly where the image is running or setting up networking to that image. It manages the entire cluster, and is responsible for:

- Managing how much available compute resources there are in the entire cluster
- Deciding which machine a process should run on
- Starting and monitoring that process on the machine
- Creating a means to route to that process
- Provisioning any required cloud resources needed by that process instance
- Handling the failure of a machine

Kubernetes is entirely declarative, and is responsible for enforcing the state of your environment. It provides a superb abstraction between "the things the developer should know about" and "the gory detail the sysadmin needs to know about". We're going to use it because it dramatically reduces the impact of failure for any one component. If a process dies, it's restarted. If a machine dies, work is scheduled elsewhere. If Kubernetes dies, you're screwed -- but at that point, you were screwed anyway.

Getting Started
---------------

Our own, tidy workshop
""""""""""""""""""""""

Most of the resources in Kubernetes operate in the context of a namespace. A namespace prevents collisions between applications that need to be discovered, lets us sets some resource limits and (coming soon) network policy. To provision anything, we have to have a namespace to put it. `Check out the namespace docs for more information.`_

Kubernetes creates resources based on text configuration, in either JSON or Yaml. I like Yaml, so we're going to use that. So, let's get started, and create a namespace resource. Create a file called `20-m1onk8s-littleman-co.ns.yml` with the following content:

.. Code:: yaml

  ---
  # Generated by Boilr at Wed, 20 Jul 2016 20:50:37 AEST
  apiVersion: "v1"
  kind: "Namespace"
  metadata:
    name: "m1onk8s-littleman-co"
    # See http://blog.kubernetes.io/2016/04/Kubernetes-Network-Policy-APIs.html
    # net.alpha.kubernetes.io/network-isolation: "off"

You'll notice a few things about this file:

#. It's got a comment that indicates it's generated. I'm too lazy to generate them myself, so I use a templating tool called `boilr`_. If you like, the templates are available on `the littleman.co GitHub`_.
#. "`net.alpha.kubernetes.io/network-isolation`_: "off"" is commented out. Alpha resources are not available on GKE; when this feature is beta, I'll try and remember to update this.
#. The file is prefixed with the number 20. We're applying lots of configuration at once, and this number determines what order to apply the configuration in.

Simple Applications
"""""""""""""""""""

There are pre-build images for MySQL and Redis that can be deployed as is, and require very little effort on the part of the developer. We're going to start with those, as Kubernetes has quite the learning curve, and it's nice to start slow.

The way I like to get applications running on Kubernetes is to have:

- A `deployment`_ artifact: Something to indicate what to run on Kubernetes, and how many copies.
- A `service`_ artifact: Something to indicate how to route things on the network, and to where.

We'll start with the deployment. The deployment I'm using is below. I've heavily commented it, to explain what each constituent part is for.

.. Code:: yaml

  ---
  # Generated by boilr at Wed, 20 Jul 2016 20:55:37 AEST
  # Kubernetes separates its artifacts into revisions, with Alpha, Beta, and standard. The apiVersion tag specifies where Kubernetes should look for this object definition.
  apiVersion: "extensions/v1beta1"
  # Well, it's a deployment, as mentioned.
  kind: "Deployment"
  metadata:
    # The labels are used for grouping tasks of resource, such as for service discovery later.
    labels:
      application: "redis"
      role: "cache"
    # How to reference this resource going forward
    name: "cache"
    # Where to put this resource
    namespace: "m1onk8s-littleman-co"
  spec:
    # How many instances of the application we want to run on the cluster. All applications can be horizontally scaled, however, in this case we're running a stateful Redis instance, and it's not so easy to scale. We'll stick to one.
    replicas: 1
    selector:
      # The deployment artifact will create a "replica set", which manages how many pods are running at any given time. Kubernetes matches the labels of the pods, defined later, with these pods to reconcile the pods the replica set is looking for with the pods in the cluster.
      matchLabels:
        application: "redis"
        role: "cache"
    strategy:
      # How to push new versions of the application. In this case, we're allowing {n}+- 1 container, where n = 1 (defined earlier in replicas). Rolling Update is the only supported deployment mechanism at the moment.
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: "RollingUpdate"
    template:
      metadata:
        labels:
          application: "redis"
          role: "cache"
      spec:
        # Todo: Actually define a PD to keep state
        # Here's where we define what volumes our docker container will have mounted. For those familar with Docker, this will be straight forward - Just know that Kubernetes lets us use a variety of storage mechanism, rather then just HOST file paths (below, we're using Google Persistent Disks).
        # volumes:
        # - name: "cache-etc-conf-d"
        # configMap:
        #   name: "cache-etc-conf-d"
        # - name: "cache-data"
        #   hostPath:
        #     path: /data/cache/
        containers:
          # Our application! Here, we're running the official redis:3.2.1-alpine container. There's not much to it, except to say that it's a redis instance running on the Apline Linux root filesystem.
        - name: "redis"
          # The docker image to use
          image: "redis:3.2.1-alpine"
          # Kubernetes will automatically pull the image onto the node that needs to run it. However, if you use the same docker image tag (for example, 'latest') and update the image, Kubernetes won't check back upstream unless you tell it with "imagePullPolicy: Always". Note: I think this is a tremendously bad idea, as different images will be updated at different times.
          imagePullPolicy: "IfNotPresent"
          # Each node has a finite amount of resource, and each application uses an amount of resource. We should (in theory) have a good idea how much resource each instance of our application will require. The below configuration allows us to "reserve" the resources required - In this case, 100m (.1) of a CPU, and 64mb of ram. I'm not too sure what the difference is yet - We're learning about this together.
          resources:
            limits:
              cpu: "100m"
              memory: "64Mi"
            requests:
              cpu: "100m"
              memory: "64Mi"
          # This is far as I'll get tonight. I'll come back another day and blog more!
          ports:
          - containerPort: 6379
            protocol: "TCP"
            name: "redis"
          # volumeMounts:
          # - name: "cache-etc-conf-d"
          #   readOnly: true
          #   mountPath: "/etc/cache/conf.d"
          # - name: "cache-data
          #   readOnly: false
          #   mountPath: "/data"
          livenessProbe:
            tcpSocket:
              port: 6379
            initialDelaySeconds: 1
            timeoutSeconds: 5
          readinessProbe:
            tcpSocket:
              port: 6379
            initialDelaySeconds: 1
            timeoutSeconds: 5
        restartPolicy: "Always"
        securityContext: {}


.. _boilr: https://github.com/boilr
.. _Check out the namespace docs for more information.: http://kubernetes.io/docs/user-guide/namespaces/
.. _deployment: http://kubernetes.io/docs/user-guide/deployments/
.. _service: http://kubernetes.io/docs/user-guide/services/
.. _net.alpha.kubernetes.io/network-isolation: http://blog.kubernetes.io/2016/04/Kubernetes-Network-Policy-APIs.html
.. _the littleman.co GitHub: https://github.com/littlemanco/

Referenecs
----------

I learned things during this too! I had previously never applied resource limits for example.

- http://kubernetes.io/docs/admin/resourcequota/walkthrough/
- http://kubernetes.io/docs/user-guide/managing-deployments/
